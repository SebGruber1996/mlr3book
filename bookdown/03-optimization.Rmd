# Model Optimization {#model-optim}

**Model Tuning**

Even though machine learning algorithms have default values set for their hyperparameters, these need to be changed by the user to achieve optimal performance on the given dataset.
A manual selection of hyperparameter values is not recommended as this approach rarely leads to an optimal performance.
This is why a data-driven optimization of hyperparameters (= [tuning](#tuning)) should be conducted.
In order to tune a machine learning algorithm, one has to specify:

* the search space
* the optimization algorithm (aka tuning method)
* an evaluation method, i.e., a resampling strategy and a performance measure

In summary, the sub-chapter on tuning illustrates hyperparameter selection, how to pick an optimizing algorithm and how to automate tuning using mlr3.
This sub-chapter requires the package `mlr3-tuning`, an extension package which supports hyperparameter tuning.

**Feature Selection**

The second part of this chapter explains [feature selection](#fs).
The objective of feature selection is to fit the sparse dependent of a model on a subset of available data features in the most suitable manner.
Feature selection can enhance the interpretability of the model, speed up model fitting and improve the learner performance by reducing noise in the data.
Different approaches exist to identify the relevant features.

In the sub-chapter on [feature selection](#fs), two approaches are emphasized:

* Feature selection using [filter](#fs-filter) algorithms
* Feature selection using the so called [wrapper methods](#fs-wrapper)

A third approach, feature selection via [ensemble filters](#fs-ensemble), is introduced subsequently.
The implementation of all three approaches in mlr3 is showcased using the extension-package `mlr3filters`.

**Nested Resampling**

In order to get a good estimate of generalization performance and avoid data leakage, both an outer (performance) and an inner (tuning/feature selection) resampling process are necessary.

The sub-section [nested resampling](#nested-resampling) will provide instructions on how to implement nested resampling in mlr3, accounting for both inner and outer resampling.


## Hyperparameter Tuning {#tuning}

Hyperparameter tuning is supported via the extension package `r mlr_pkg("mlr3tuning")`.
The heart of `r mlr_pkg("mlr3tuning")` are the R6 classes

* `r ref("TuningInstance")`: This class describes the tuning problem and stores results.
* `r ref("Tuner")`: This class is the base class for implementations of tuning algorithms.

### The `TuningInstance` Class

The following sub-section examines the optimization of a simple classification tree on the `r ref("mlr_tasks_pima", text = "Pima Indian Diabetes")` data set.

```{r 03-optimization-001}
task = tsk("pima")
print(task)
```

We use the classification tree from `r cran_pkg("rpart")` and choose a subset of the hyperparameters we want to tune.
This is often referred to as the "tuning space".

```{r 03-optimization-002}
learner = lrn("classif.rpart")
learner$param_set
```

Here, we opt to tune two parameters:

* The complexity `cp`
* The termination criterion `minsplit`

As the tuning space has to be bound, one has to set lower and upper bounds:

```{r 03-optimization-003}
library(paradox)
tune_ps = ParamSet$new(list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)
))
tune_ps
```

Next, we need to define how to evaluate the performance.
For this, we need to choose a `r ref("Resampling", text = "resampling strategy")` and a `r ref("Measure", text = "performance measure")`.

```{r 03-optimization-004}
hout = rsmp("holdout")
measure = msr("classif.ce")
```

Finally, one has to determine the budget available to solve this tuning instance.
This is done by selecting one of the available `r ref("Terminator", text = "Terminators")`:

* Terminate after a given time (`r ref("TerminatorClockTime")`)
* Terminate after a given amount of iterations (`r ref("TerminatorEvals")`)
* Terminate after a specific performance is reached (`r ref("TerminatorPerfReached")`)
* Terminate when tuning does not improve (`r ref("TerminatorStagnation")`)
* A combination of the above in an *ALL* or *ANY* fashion, using `r ref("TerminatorCombo")`

For this short introduction, we grant a budget of 20 evaluations and then put everything together into a `r ref("TuningInstance")`:

```{r 03-optimization-005}
library(mlr3tuning)

evals20 = term("evals", n_evals = 20)

instance = TuningInstance$new(
  task = task,
  learner = learner,
  resampling = hout,
  measures = measure,
  param_set = tune_ps,
  terminator = evals20
)
print(instance)
```

To start the tuning, we still need to select how the optimization should take place - in other words, we need to choose the **optimization algorithm** via the `r ref("Tuner")` class.

### The `Tuner` Class

The following algorithms are currently implemented in `r mlr_pkg("mlr3tuning")`:

* Grid Search (`r ref("TunerGridSearch")`)
* Random Search (`r ref("TunerRandomSearch")`) [@bergstra2012]
* Generalized Simulated Annealing (`r ref("TunerGenSA")`)

In this example, we will use a simple grid search with a grid resolution of 10:

```{r 03-optimization-006}
tuner = tnr("grid_search", resolution = 5)
```

Since we have only numeric parameters, `r ref("TunerGridSearch")` will create a grid of equally-sized steps between the respective upper and lower bounds.
As we have two hyperparameters with a resolution of 5, the two-dimensional grid consists of $5^2 = 25$ configurations.
Each configuration serves as hyperparameter setting for the classification tree and triggers a 3-fold cross validation on the task.
All configurations will be examined by the tuner (in a random order), until either all configurations are evaluated or the `r ref("Terminator")` signals that the budget is exhausted.

### Triggering the Tuning

To start the tuning, we simply pass the `r ref("TuningInstance")` to the `$tune()` method of the initialized `r ref("Tuner")`.
The tuner proceeds as follow:

1. The `r ref("Tuner")` proposes at least one hyperparameter configuration (the `r ref("Tuner")` and may propose multiple points to improve parallelization, which can be controlled via the setting `batch_size`).
2. For each configuration, a `r ref("Learner")` is fitted on `r ref("Task")` using the provided `r ref("Resampling")`.
   The results are combined with other results from previous iterations to a single `r ref("BenchmarkResult")`.
3. The `r ref("Terminator")` is queried if the budget is exhausted.
   If the budget is not exhausted, restart with 1) until it is.
4. Determine the configuration with the best observed performance.
5. Return a named list with the hyperparameter settings (`"values"`) and the corresponding measured performance (`"performance"`).

```{r 03-optimization-007}
result = tuner$tune(instance)
print(result)
```

One can investigate all resamplings which where undertaken, using the `$archive()` method of the `r ref("TuningInstance")`.
Here, one just extracts the performance values and the hyperparameters:

```{r 03-optimization-008}
instance$archive(unnest = "params")[, c("cp", "minsplit", "classif.ce")]
```

In sum, the grid search evaluated 20/25 different configurations of the grid in a random order before the `r ref("Terminator")` stopped the tuning.

Now the optimized hyperparameters can take the previously created `r ref("Learner")`, set the returned hyperparameters and [train](#train-predict) it on the full dataset.

```{r 03-optimization-009}
learner$param_set$values = instance$result$params
learner$train(task)
```

The trained model could now be used to make a prediction on external data.
Note that predicting on observations present in the `task`, is statistically bias and should be avoided, as the model has seen these observations already during tuning.
Hence, the resulting performance measure would be over-optimistic.
Instead, to get unbiased performance estimates for the current task, [nested resampling](#nested-resamling) is required.

### Automating the Tuning {#autotuner}

The `r ref("AutoTuner")` wraps a learner and augments it with an automatic tuning for a given set of hyperparameters.
Because the `r ref("AutoTuner")` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner.
Analogously to the previous subsection, a new classification tree learner is created.
This classification tree learner automatically tunes the parameters `cp` and `minsplit` using an inner resampling (holdout).
We create a terminator which allows 10 evaluations, and use a simple random search as tuning algorithm:

```{r 03-optimization-010}
library(paradox)
library(mlr3tuning)

learner = lrn("classif.rpart")
resampling = rsmp("holdout")
measures = msr("classif.ce")
tune_ps = ParamSet$new(list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)
))
terminator = term("evals", n_evals = 10)
tuner = tnr("random_search")

at = AutoTuner$new(
  learner = learner,
  resampling = resampling,
  measures = measures,
  tune_ps = tune_ps,
  terminator = terminator,
  tuner = tuner
)
at
```

We can now use the learner like any other learner, calling the `$train()` and `$predict()` method.
This time however, we pass it to `r ref("benchmark()")` to compare the tuner to a classification tree without tuning.
This way, the `r ref("AutoTuner")` will do its resampling for tuning on the training set of the respective split of the outer resampling.
The learner then predicts using the test set of the outer resampling.
This yields unbiased performance measures, as the observations in the test set have not been used during tuning or fitting of the respective learner.
This is called [nested resampling](#nested-resampling).

To compare the tuned learner with the learner using its default, we can use `r ref("benchmark()")`:

```{r 03-optimization-011}
grid = benchmark_grid(
  task = tsk("pima"),
  learner = list(at, lrn("classif.rpart")),
  resampling = rsmp("cv", folds = 3)
)
bmr = benchmark(grid)
bmr$aggregate(measures)
```

Note that we do not expect any differences here compared to the non-tuned approach for multiple reasons:

* the task is too easy
* the task is rather small, and thus prone to overfitting
* the tuning budget (10 evaluations) is small
* `r cran_pkg("rpart")` does not benefit that much from tuning

### Tuning with Hyperband {#hyperband}

Besides the more traditional tuning methods listed above, the ecosystem around `r mlr_pkg("mlr3")` offers another procedure for hyperparameter optimization called Hyperband implemented in the `r gh_pkg("mlr-org/mlr3hyperband")` package.
Hyperband is a budget-oriented procedure, weeding out suboptimally performing configurations early on during a partially sequential training process, increasing tuning efficiency as a consequence.
For this, a combination of incremental resource allocation and early stopping is used: As optimization progresses, computational resources are increased for more promising configurations, while less promising ones are terminated early.
To give an introductional analogy, imagine two horse trainers are given eight untrained horses.
Both trainers want to win the upcoming race, but they are only given 32 units of food.
Given that each horse can be fed up to 8 units food ("maximum budget" per horse), there is not enough food for all the horses.
It is critical to identify the most promising horses early, and give them enough food to improve.
So, the trainers need to develop a strategy to split up the food in the best possible way.
The first trainer is very optimistic and wants to explore the full capabilities of a horse, because he does not want to pass a judgement on a horse's performance unless it has been fully trained.
because he does not want to a judgement before they are not fully trained.
So, he divides his budget by the maximum amount he can give to a horse (lets say eigth, so $32 / 8 = 4$) and randomly picks four horses -- his budget simply is not enough to fully train more.
Those four horses are then trained to their full capabilities, while the rest is set free.
This way, the trainer is confident about choosing the best out of the four trained horses, but he might have overlooked the horse with the highest potentialsince he only focused on half of them.
The other trainer is more creative and develops a different strategy.
He thinks, if a horse is not performing well at the beginning, it will also not improve after further training.
Based on this assumption, he decides to give one unit of food to each horse and observes how they develop.
After the initial food is consumed, he checks their performance and kicks the slowest half out of his training regime.
Then, he increases the available food for the remaining, further trains them until the food is consumed again, only to kick out the worst half once more.
He repeats this until the one remaining horse gets the rest of the food.
This means only one horse is fully trained, but on the flip side, he was able to start training with all eight horses.
On race day, all the horses are put on the starting line.
But which trainer will have the winning horse?
The one, who tried to train a maximum amount of horses to their fullest?
Or the other one, who made assumptions about the training progress of his horses?
How the training phases may possibly look like is visualized in figure \@ref(fig:fighorses).

```{r 03-optimization-012, fighorses, eval = TRUE, echo = FALSE, fig.show='hold', out.width = '99%', fig.align = 'center', fig.cap = "Visulization of how the training processes may look like. The left plot depicts the training of the non-selective trainer, while the right one shows the selective trainer.", warning=FALSE}
knitr::include_graphics("images/horse_training1.png")
```

Hyperband works very similar in some ways, but also different in others.
It is not embodied by one of the trainers in our analogy, but more by the person, who would pay them.
Hyperband consists of several brackets, each bracket corresponding to a trainer, and we do not care about horses but about hyperparameter configurations of a machine learning algorithm.
The budget is not in terms of food, but in terms of a hyperparameter of the learner that scales in some way with the computational effort.
An example is the number of epochs we train a neural network, or the number of iterations in boosting.
Furthermore, there are not only two brackets (or trainers), but several, each placed at a unique spot between fully explorative of later training stages and extremely selective, equal to higher exploration of early training stages.
The level of selection aggressiveness is handled by a user-defined parameter called $\eta$.
So, $1/\eta$ is the fraction of remaining configurations after a bracket removes his worst performing ones, but $\eta$ is also the factor by that the budget is increased for the next stage.
Because there is a different maximum budget per configuration that makes sense in different scenarios, the user also has to set this as the $R$ parameter.
No further parameters are required for Hyperband -- the full required budget across all brackets is indirectly given by $$(\lfloor \log_{\eta}{R} \rfloor + 1)^2 * R$$ [@Li2016].
To give an idea how a full bracket layout might look like for a specific $R$ and $\eta$, a quick overview is given in the following table.

```{r 03-optimization-013, eval = TRUE, echo = FALSE}
eta = 2
R = 8
result = data.frame()
smax = floor(log(R, eta))
B = (smax + 1) * R

# outer loop - iterate over brackets
for (s in smax:0) {

  n = ceiling((B/R) * ((eta^s)/(s+1)))
  r = R * eta^(-s)

  # inner loop - iterate over bracket stages
  for (i in 0:s) {

    ni = floor(n * eta^(-i))
    ri = r * eta^i
    result = rbind(result, c(smax - s + 1, i + 1, ri, ni))
  }
}

names(result) = c("bracket", "stage", "budget", "n")

knitr::kable(
  list(
	result[1:4, 2:4],
	data.frame(stage = 1:3, budget = 2^(1:3), n = c(6,3,1)),
	data.frame(stage = 1:2, budget = c(4,8), n = c(4,2)),
	data.frame(stage = 1, budget = 8, n = 4)
  ),
  booktabs = TRUE,
  caption = "Hyperband layout for $\\eta = 2$ and $R = 8$, consisting of four brackets with $n$ as the amount of active configurations."
)
```

Of course, early termination based on a performance criterion may be disadvantageous if it is done too aggressively in certain scenarios.
A learner to jumping radically in its estimated performance during the training phase may get the best configurations canceled too early, simply because they do not improve quickly enough compared to others.
In other words, it is often unclear beforehand if having an high amount of configurations $n$, that gets aggressively discarded early, is better than having a high budget $B$ per configuration.
The arising tradeoff, that has to be made, is called the "$n$ versus $B/n$ problem".
To create a balance between selection based on early training performance versus exploration of training performances in later training stages, $\lfloor \log_{\eta}{R} \rfloor + 1$ brackets are constructed with an associated set of varyingly sized configurations.
Thus, some brackets contain more configurations, with a small initial budget.
In these, a lot are discarded after having been trained for only a short amount of time, corresponding to the selective trainer in our horse analogy.
Others are constructed with fewer configurations, where discarding only takes place after a significant amount of budget was consumed.
The last bracket usually never discards anything, but also starts with only very few configurations -- this is equivalent to the trainer explorative of later stages.
The former corresponds high $n$, while the latter high $B/n$.
Even though different brackets are initialized with a different amount of configurations and different initial budget sizes, each bracket is assigned (approximately) the same budget $(\lfloor \log_{\eta}{R} \rfloor + 1) * R$.

The configurations at the start of each bracket are initialized by random, often uniform sampling.
Note that currently all configurations are trained completely from the beginning, so no online updates of models from stage to stage is happening.

To identify the budget for evaluating Hyperband, the user has to specify explicitly which hyperparameter of the learner influences the budget by extending a single hyperparameter in the `r ref("ParamSet")` with an argument (`tags = "budget"`), like in the following snippet:

```{r 03-optimization-014}
library(paradox)

# Hyperparameter subset of XGBoost
params = list(
  ParamInt$new("nrounds", lower = 1, upper = 16, tags = "budget"),
  ParamFct$new("booster", levels = c("gbtree", "gblinear", "dart"))
)
```

Thanks to the broad ecosystem of the `r mlr_pkg("mlr3verse")` a learner does not require a natural budget parameter.
A typical case of this would be decision trees.
By using subsampling as preprocessing with `r mlr_pkg("mlr3pipelines")`, we can work around a lacking budget parameter.

```{r 03-optimization-015}
library(mlr3hyperband)
library(mlr3pipelines)
set.seed(123)

# extend "classif.rpart" with "subsampling" as preprocessing step
ll = po("subsample") %>>% lrn("classif.rpart")

# extend hyperparameters of "classif.rpart" with subsampling fraction as budget
params = list(
  ParamDbl$new("classif.rpart.cp", lower = 0.001, upper = 0.1),
  ParamInt$new("classif.rpart.minsplit", lower = 1, upper = 10),
  ParamDbl$new("subsample.frac", lower = 0.1, upper = 1, tags = "budget")
)
```

We can now plug the new learner with the extended hyperparameter set into a `r ref("TuningInstance")` the same way as usual.
Naturally, Hyperband terminates once all of its brackets are evaluated, so a `r ref("Terminator")` in the tuning instance acts as an upper bound and should be only set to a low value if one is unsure of how long hyperband will take to finish under the given settings.

```{r 03-optimization-016}
inst = TuningInstance$new(
  tsk("iris"),
  ll,
  rsmp("holdout"),
  msr("classif.ce"),
  ParamSet$new(params),
  term("evals", n_evals = 100000L)
)
```

Now, we initialize a new instance of the `r ref("mlr3hyperband::TunerHyperband")` class and start tuning with it.

```{r 03-optimization-017}
tuner = TunerHyperband$new(eta = 3)
tuner$tune(inst)
```

To receive the results of each sampled configuration, we simply run the following snippet.

```{r 03-optimization-018}
inst$archive(unnest = "params")[, c(
  "subsample.frac",
  "classif.rpart.cp",
  "classif.rpart.minsplit",
  "classif.ce"
)]
```

Additionally, we can also only extract the best performing configuration.

```{r 03-optimization-019}
inst$best()
```

If you are familiar with the original paper, you may have wondered how we just used Hyperband with a parameter ranging from `0.1` to `1.0` [@Li2016].
The answer is, with the help the internal rescaling of the budget parameter.
`r gh_pkg("mlr-org/mlr3hyperband")` automatically divides the budget parameters boundaries with its lower bound, ending up with a budget range starting again at `1`, like it is the case originally.
If we want an overview of what bracket layout Hyperband created and how the rescaling in each bracket worked, we can print a compact table to see this information.

```{r 03-optimization-020}
tuner$info
```

In the traditional way, Hyperband uses uniform sampling to receive a configuration sample at the start of each bracket.
But it is also possible to define a custom `r ref("Sampler")` for each hyperparameter.

```{r 03-optimization-021}
library(mlr3learners)
set.seed(123)

params = list(
  ParamInt$new("nrounds", lower = 1, upper = 16, tag = "budget"),
  ParamDbl$new("eta",     lower = 0, upper = 1),
  ParamFct$new("booster", levels = c("gbtree", "gblinear", "dart"))
)

inst = TuningInstance$new(
  tsk("iris"),
  lrn("classif.xgboost"),
  rsmp("holdout"),
  msr("classif.ce"),
  ParamSet$new(params),
  term("evals", n_evals = 100000L)
)

# beta distribution with alpha = 2 and beta = 5
# categorical distribution with custom probabilities
sampler = SamplerJointIndep$new(list(
  Sampler1DRfun$new(params[[2]], function(n) rbeta(n, 2, 5)),
  Sampler1DCateg$new(params[[3]], prob = c(0.2, 0.3, 0.5))
))
```

Then, the defined sampler has to be given as an argument during instance creation.
Afterwards, the usual tuning can proceed.

```{r 03-optimization-022}
tuner = TunerHyperband$new(eta = 2, sampler = sampler)
tuner$tune(inst)

inst$best()
```

Furthermore, we extended the original alogrithm, to make it also possible to use `r gh_pkg("mlr-org/mlr3hyperband")` for multi-objective optimization.
To do this, simply specify more measures in the tuninginstance and run the rest as usual.

```{r 03-optimization-023}
inst = TuningInstance$new(
  tsk("pima"),
  lrn("classif.xgboost"),
  rsmp("holdout"),
  list(msr("classif.tpr"), msr("classif.fpr")),
  ParamSet$new(params),
  term("evals", n_evals = 100000L)
)

tuner = TunerHyperband$new(eta = 4)
tuner$tune(inst)
```

Keep in mind that `$best()` may not work as desired in the multi-objective case as it always returns only one result w.r.t. a single measure.


## Feature Selection / Filtering {#fs}

Often, data sets include a large number of features.
The technique of extracting a subset of relevant features is called "feature selection".
The objective of feature selection is to fit the sparse dependent of a model on a subset of available data features in the most suitable manner.
Feature selection can enhance the interpretability of the model, speed up the learning process and improve the learner performance.
Different approaches exist to identify the relevant features.
In the literature two different approaches are emphasized:
One is called [Filtering](#fs-filtering) and the other approach is often referred to as feature subset selection or [wrapper methods](#fs-wrapper).

What are the differences [@chandrashekar2014]?

* **Filter**: An external algorithm computes a rank of the variables (e.g. based on the correlation to the response).
  Then, features are subsetted by a certain criteria, e.g. an absolute number or a percentage of the number of variables.
  The selected features will then be used to fit a model (with optional hyperparameters selected by tuning).
  This calculation is usually cheaper than “feature subset selection” in terms of computation time.
* **Feature subset selection**: Here, no ranking of features is done.
  Features are selected by a (random) subset of the data.
  Then, a model is fitted and the performance is checked.
  This is done for a lot of feature combinations in a cross-validation (CV) setting and the best combination is reported.
  This method is very computational intense as a lot of models are fitted.
  Also, strictly all these models would need to be tuned before the performance is estimated which would require an additional nested level in a CV setting.
  After all this, the selected subset of features is again fitted (with optional hyperparameters selected by tuning).

There is also a third approach which can be attributed to the "filter" family:
The embedded feature-selection methods of some `r ref("Learner")`.
Read more about how to use these in section [embedded feature-selection methods](#fs-embedded).

[Ensemble filters]({#fs-ensemble}) built upon the idea of stacking single filter methods.
These are not yet implemented.

All feature selection related functionality is implemented via the extension package `r gh_pkg("mlr-org/mlr3filters")`.

### Filters {#fs-filter}

Filter methods assign an importance value to each feature.
Based on these values the features can be ranked and a feature subset can be selected.
There is a list of all implemented filter methods in the [Appendix](#list-filters).

### Calculating filter values {#fs-calc}

Currently, only classification and regression tasks are supported.

The first step it to create a new R object using the class of the desired filter method.
Each object of class `Filter` has a `.$calculate()` method which calculates the filter values and ranks them in a descending order.

```{r 03-optimization-024}
library(mlr3filters)
filter = FilterJMIM$new()

task = tsk("iris")
filter$calculate(task)

as.data.table(filter)
```

Some filters support changing specific hyperparameters.
This is done similar to setting hyperparameters of a `r ref("Learner")` using `.$param_set$values`:

```{r 03-optimization-025}
filter_cor = FilterCorrelation$new()
filter_cor$param_set

# change parameter 'method'
filter_cor$param_set$values = list(method = "spearman")
filter_cor$param_set
```

Rather than taking the "long" R6 way to create a filter, there is also a built-in shorthand notation for filter creation

```{r 03-optimization-026}
filter = flt("cmim")
filter
```

### Variable Importance Filters {#fs-var-imp-filters}

All `r ref("Learner")` with the property "importance" come with integrated feature selection methods.

You can find a list of all learners with this property in the [Appendix](#fs-filter-embedded-list).

For some learners the desired filter method needs to be set during learner creation.
For example, learner `classif.ranger` (in `r mlr_pkg("mlr3learners")` comes with multiple integrated methods.
See the help page of `r ref("ranger::ranger")`.
To use method "impurity", you need to set the filter method during construction.

```{r 03-optimization-027}
library(mlr3learners)
lrn = lrn("classif.ranger", importance = "impurity")
```

Now you can use the `r ref("mlr3filters::FilterImportance")` class for algorithm-embedded methods to filter a `r ref("Task")`.

```{r 03-optimization-028}
library(mlr3learners)

task = tsk("iris")
filter = flt("importance", learner = lrn)
filter$calculate(task)
head(as.data.table(filter), 3)
```

### Ensemble Methods {#fs-ensemble}

```{block, type='warning'}
Work in progress :)
```

### Wrapper Methods {#fs-wrapper}

```{block, type='warning'}
Work in progress :) - via package _mlr3fswrap_
```

## Nested Resampling {#nested-resampling}

In order to obtain unbiased performance estimates for learners, all parts of the model building (preprocessing and model selection steps) should be included in the resampling, i.e., repeated for every pair of training/test data.
For steps that themselves require resampling like hyperparameter tuning or feature-selection (via the wrapper approach) this results in two nested resampling loops.

```{r 03-optimization-029, echo = FALSE, out.width="98%"}
knitr::include_graphics("images/nested_resampling.png")
```

The graphic above illustrates nested resampling for parameter tuning with 3-fold cross-validation in the outer and 4-fold cross-validation in the inner loop.

In the outer resampling loop, we have three pairs of training/test sets.
On each of these outer training sets parameter tuning is done, thereby executing the inner resampling loop.
This way, we get one set of selected hyperparameters for each outer training set. Then the learner is fitted on each outer training set using the corresponding selected hyperparameters and its performance is evaluated on the outer test sets.

In `r gh_pkg("mlr-org/mlr3")`, you can get nested resampling for free without programming any looping by using the `r ref("mlr3tuning::AutoTuner")` class.
This works as follows:

1. Generate a wrapped Learner via class `r ref("mlr3tuning::AutoTuner")` or `mlr3filters::AutoSelect` (not yet implemented).
2. Specify all required settings - see section ["Automating the Tuning"](#autotuner) for help.
3. Call function `r ref("resample()")` or `r ref("benchmark()")` with the created `r ref("Learner")`.

You can freely combine different inner and outer resampling strategies.

A common setup is prediction and performance evaluation on a fixed outer test set. This can be achieved by passing the `r ref("Resampling")` strategy (`rsmp("holdout")`) as the outer resampling instance to either `r ref("resample()")` or `r ref("benchmark()")`.

The inner resampling strategy could be a cross-validation one (`rsmp("cv")`) as the sizes of the outer training sets might differ.
Per default, the inner resample description is instantiated once for every outer training set.

Nested resampling is computationally expensive.
For this reason in the examples shown below, we use relatively small search spaces and a low number of resampling iterations.
In practice, you normally have to increase both.
As this is computationally intensive you might want to have a look at the section [Parallelization](#parallelization).

### Execution

To optimize hyperparameters or conduct features-selection in a nested resampling you need to create learners using either:

* the `r ref("AutoTuner")` class, or
* the `mlr3filters::AutoSelect` class (not yet implemented)

We use the example from section ["Automating the Tuning"](#autotuner) and pipe the resulting learner into a `r ref("resample()")` call.

```{r 03-optimization-030}
library(mlr3tuning)
task = tsk("iris")
learner = lrn("classif.rpart")
resampling = rsmp("holdout")
measures = msr("classif.ce")
param_set = paradox::ParamSet$new(
  params = list(paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1)))
terminator = term("evals", n_evals = 5)
tuner = tnr("grid_search", resolution = 10)

at = AutoTuner$new(learner, resampling, measures = measures,
  param_set, terminator, tuner = tuner)
```

Now construct the `r ref("resample()")` call:

```{r 03-optimization-031}
resampling_outer = rsmp("cv", folds = 3)
rr = resample(task = task, learner = at, resampling = resampling_outer)
```

### Evaluation {#rr-eval}

With the created `r ref("ResampleResult")` we can now inspect the executed resampling iterations more closely.
See also section [Resampling](#resampling) for more detailed information about `r ref("ResampleResult")` objects.

For example, we can query the aggregated performance result:

```{r 03-optimization-032}
rr$aggregate()
```

<!-- We can also query the tuning result of any learner using the `$tune_path` field of the `r ref("AutoTuner")` class stored in the `r ref("ResampleResult")` container `rr`. -->

<!-- ```{block, type="caution"} -->
<!-- Note: This only works if `store_bmr` was set to `TRUE` in the `AutoTuner` object. -->
<!-- ``` -->

<!-- ```{r 02-nested-resamp-005, eval = FALSE} -->
<!-- # FIXME: not yet done -->
<!-- rr$learners[[1]]$tune_path -->
<!-- ``` -->

Check for any errors in the folds during execution (if there is not output, warnings or errors recorded, this is an empty `data.table()`:

```{r 03-optimization-033}
rr$errors
```

Or take a look at the confusion matrix of the joined predictions:

```{r 03-optimization-034}
rr$prediction()$confusion
```

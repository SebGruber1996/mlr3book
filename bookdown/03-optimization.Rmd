# Model Optimization {#model-optim}

**Model Tuning**

Even though machine learning algorithms have default values set for their hyperparameters, these need to be changed by the user to achieve optimal performance on the given dataset.
A manual selection of hyperparameter values is not recommended as this approach rarely leads to an optimal performance.
This is why a data-driven optimization of hyperparameters (= [tuning](#tuning)) should be conducted.
In order to tune a machine learning algorithm, one has to specify:

* the search space
* the optimization algorithm (aka tuning method)
* an evaluation method, i.e., a resampling strategy and a performance measure

In summary, the sub-chapter on tuning illustrates hyperparameter selection, how to pick an optimizing algorithm and how to automate tuning using mlr3.
This sub-chapter requires the package `mlr3-tuning`, an extension package which supports hyperparameter tuning.

**Feature Selection**

The second part of this chapter explains [feature selection](#fs).
The objective of feature selection is to fit the sparse dependent of a model on a subset of available data features in the most suitable manner.
Feature selection can enhance the interpretability of the model, speed up model fitting and improve the learner performance by reducing noise in the data.
Different approaches exist to identify the relevant features.

In the sub-chapter on [feature selection](#fs), two approaches are emphasized:

* Feature selection using [filter](#fs-filter) algorithms
* Feature selection using the so called [wrapper methods](#fs-wrapper)

A third approach, feature selection via [ensemble filters](#fs-ensemble), is introduced subsequently.
The implementation of all three approaches in mlr3 is showcased using the extension-package `mlr3filters`.

**Nested Resampling**

In order to get a good estimate of generalization performance and avoid data leakage, both an outer (performance) and an inner (tuning/feature selection) resampling process are necessary.

The sub-section [nested resampling](#nested-resampling) will provide instructions on how to implement nested resampling in mlr3, accounting for both inner and outer resampling.


## Hyperparameter Tuning {#tuning}

Hyperparameter tuning is supported via the extension package `r mlr_pkg("mlr3tuning")`.
The heart of `r mlr_pkg("mlr3tuning")` are the R6 classes

* `r ref("TuningInstance")`: This class describes the tuning problem and stores results.
* `r ref("Tuner")`: This class is the base class for implementations of tuning algorithms.

### The `TuningInstance` Class

The following sub-section examines the optimization of a simple classification tree on the `r ref("mlr_tasks_pima", text = "Pima Indian Diabetes")` data set.

```{r 03-optimization-001}
task = tsk("pima")
print(task)
```

We use the classification tree from `r cran_pkg("rpart")` and choose a subset of the hyperparameters we want to tune.
This is often referred to as the "tuning space".

```{r 03-optimization-002}
learner = lrn("classif.rpart")
learner$param_set
```

Here, we opt to tune two parameters:

* The complexity `cp`
* The termination criterion `minsplit`

As the tuning space has to be bound, one has to set lower and upper bounds:

```{r 03-optimization-003}
library(paradox)
tune_ps = ParamSet$new(list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)
))
tune_ps
```

Next, we need to define how to evaluate the performance.
For this, we need to choose a `r ref("Resampling", text = "resampling strategy")` and a `r ref("Measure", text = "performance measure")`.

```{r 03-optimization-004}
hout = rsmp("holdout")
measure = msr("classif.ce")
```

Finally, one has to determine the budget available to solve this tuning instance.
This is done by selecting one of the available `r ref("Terminator", text = "Terminators")`:

* Terminate after a given time (`r ref("TerminatorClockTime")`)
* Terminate after a given amount of iterations (`r ref("TerminatorEvals")`)
* Terminate after a specific performance is reached (`r ref("TerminatorPerfReached")`)
* Terminate when tuning does not improve (`r ref("TerminatorStagnation")`)
* A combination of the above in an *ALL* or *ANY* fashion, using `r ref("TerminatorCombo")`

For this short introduction, we grant a budget of 20 evaluations and then put everything together into a `r ref("TuningInstance")`:

```{r 03-optimization-005}
library(mlr3tuning)

evals20 = term("evals", n_evals = 20)

instance = TuningInstance$new(
  task = task,
  learner = learner,
  resampling = hout,
  measures = measure,
  param_set = tune_ps,
  terminator = evals20
)
print(instance)
```

To start the tuning, we still need to select how the optimization should take place - in other words, we need to choose the **optimization algorithm** via the `r ref("Tuner")` class.

### The `Tuner` Class

The following algorithms are currently implemented in `r mlr_pkg("mlr3tuning")`:

* Grid Search (`r ref("TunerGridSearch")`)
* Random Search (`r ref("TunerRandomSearch")`) [@bergstra2012]
* Generalized Simulated Annealing (`r ref("TunerGenSA")`)

In this example, we will use a simple grid search with a grid resolution of 10:

```{r 03-optimization-006}
tuner = tnr("grid_search", resolution = 5)
```

Since we have only numeric parameters, `r ref("TunerGridSearch")` will create a grid of equally-sized steps between the respective upper and lower bounds.
As we have two hyperparameters with a resolution of 5, the two-dimensional grid consists of $5^2 = 25$ configurations.
Each configuration serves as hyperparameter setting for the classification tree and triggers a 3-fold cross validation on the task.
All configurations will be examined by the tuner (in a random order), until either all configurations are evaluated or the `r ref("Terminator")` signals that the budget is exhausted.

### Triggering the Tuning

To start the tuning, we simply pass the `r ref("TuningInstance")` to the `$tune()` method of the initialized `r ref("Tuner")`.
The tuner proceeds as follow:

1. The `r ref("Tuner")` proposes at least one hyperparameter configuration (the `r ref("Tuner")` and may propose multiple points to improve parallelization, which can be controlled via the setting `batch_size`).
2. For each configuration, a `r ref("Learner")` is fitted on `r ref("Task")` using the provided `r ref("Resampling")`.
   The results are combined with other results from previous iterations to a single `r ref("BenchmarkResult")`.
3. The `r ref("Terminator")` is queried if the budget is exhausted.
   If the budget is not exhausted, restart with 1) until it is.
4. Determine the configuration with the best observed performance.
5. Return a named list with the hyperparameter settings (`"values"`) and the corresponding measured performance (`"performance"`).

```{r 03-optimization-007}
result = tuner$tune(instance)
print(result)
```

One can investigate all resamplings which where undertaken, using the `$archive()` method of the `r ref("TuningInstance")`.
Here, one just extracts the performance values and the hyperparameters:

```{r 03-optimization-008}
instance$archive(unnest = "params")[, c("cp", "minsplit", "classif.ce")]
```

In sum, the grid search evaluated 20/25 different configurations of the grid in a random order before the `r ref("Terminator")` stopped the tuning.

Now the optimized hyperparameters can take the previously created `r ref("Learner")`, set the returned hyperparameters and [train](#train-predict) it on the full dataset.

```{r 03-optimization-009}
learner$param_set$values = instance$result$params
learner$train(task)
```

The trained model could now be used to make a prediction on external data.
Note that predicting on observations present in the `task`, is statistically bias and should be avoided, as the model has seen these observations already during tuning.
Hence, the resulting performance measure would be over-optimistic.
Instead, to get unbiased performance estimates for the current task, [nested resampling](#nested-resamling) is required.

### Automating the Tuning {#autotuner}

The `r ref("AutoTuner")` wraps a learner and augments it with an automatic tuning for a given set of hyperparameters.
Because the `r ref("AutoTuner")` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner.
Analogously to the previous subsection, a new classification tree learner is created.
This classification tree learner automatically tunes the parameters `cp` and `minsplit` using an inner resampling (holdout).
We create a terminator which allows 10 evaluations, and use a simple random search as tuning algorithm:

```{r 03-optimization-010}
library(paradox)
library(mlr3tuning)

learner = lrn("classif.rpart")
resampling = rsmp("holdout")
measures = msr("classif.ce")
tune_ps = ParamSet$new(list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)
))
terminator = term("evals", n_evals = 10)
tuner = tnr("random_search")

at = AutoTuner$new(
  learner = learner,
  resampling = resampling,
  measures = measures,
  tune_ps = tune_ps,
  terminator = terminator,
  tuner = tuner
)
at
```

We can now use the learner like any other learner, calling the `$train()` and `$predict()` method.
This time however, we pass it to `r ref("benchmark()")` to compare the tuner to a classification tree without tuning.
This way, the `r ref("AutoTuner")` will do its resampling for tuning on the training set of the respective split of the outer resampling.
The learner then predicts using the test set of the outer resampling.
This yields unbiased performance measures, as the observations in the test set have not been used during tuning or fitting of the respective learner.
This is called [nested resampling](#nested-resampling).

To compare the tuned learner with the learner using its default, we can use `r ref("benchmark()")`:

```{r 03-optimization-011}
grid = benchmark_grid(
  task = tsk("pima"),
  learner = list(at, lrn("classif.rpart")),
  resampling = rsmp("cv", folds = 3)
)
bmr = benchmark(grid)
bmr$aggregate(measures)
```

Note that we do not expect any differences here compared to the non-tuned approach for multiple reasons:

* the task is too easy
* the task is rather small, and thus prone to overfitting
* the tuning budget (10 evaluations) is small
* `r cran_pkg("rpart")` does not benefit that much from tuning

### Tuning with Hyperband {#hyperband}

Besides the more traditional tuning methods listed above, the ecosystem around `r mlr_pkg("mlr3")` offers another procedure for hyperparameter optimization called Hyperband implemented in the `r gh_pkg("mlr-org/mlr3hyperband")` package.
Hyperband is a budget-oriented procedure, weeding out suboptimally performing configurations early on during a sequential training process, increasing tuning efficiency as a consequence.
For this, a combination of adaptive resource allocation and early stopping is used, shifting resources on more promising configurations plus terminating lesser ones the further training stages are reached.
To give an introductional analogy, imagine two horse trainers are given eight untrained horses.
Both trainers want to end up with the quickest horse to win the incoming race, but they are only given a fixed amount of budget (32 euros) to buy food.
But, besides the normal training, it is critical to give the horses enough food to improve, so the trainers need to develop a strategy to split up the food between the horses.
The first trainer is very optimistic and wants to explore the full capabilities of a horse, because he does not want to judge a horse before it is not fully trained.
So, he divides his budget by the maximum amount he can give to a horse (lets say eigth, so $32 / 8 = 4$) and randomly picks four horses -- his budget simply is not enough to fully train more.
Those four horses are then trained to their full capabilities, while the rest is sold.
The other trainer is smarter but also very pessimistic and exploiting.
He thinks, if a horse is not performing well at first, it will also not improve after further training.
Based on this assumption he develops a strategy that only gives very little food to the horses in the beginning.
After the initial food is consumed, he checks their performance and kicks the slowest half out of his training regime.
Then, he increases the available food for the remaining, further trains them until the food is consumed again, only to kick out the worst half once more.
He repeats this until the one remaining horse gets the rest of the food.
On race day, all the trained horses (even those, who did not receive full training, because the exploitative trainer takes every chance he gets) are put on the starting line.
But which trainer will have the winning horse?
The one, who tried to train a maximum amount of horses to their fullest?
Or the one, who is exploiting the training progress of his horses?
Hyperband works in a very similar way, except it is not one of the trainers in our analogy, but more the person, who paid both trainers, to end up buying the horse that finished quickest at the race.
Hyperband consists of several brackets, each bracket corresponding to a trainer.
But there are not only two trainers, but several, each placed at a unique spot between extremely explorative and extremely exploitative.
Furthermore, the input for Hyperband to ... WIP

```{r, eval = TRUE, echo = FALSE}
eta = 2
R = 8
result = data.frame()
smax = floor(log(R, eta))
B = (smax + 1) * R

# outer loop - iterate over brackets
for (s in smax:0) {

  n = ceiling((B/R) * ((eta^s)/(s+1)))
  r = R * eta^(-s)

  # inner loop - iterate over bracket stages
  for (i in 0:s) {

    ni = floor(n * eta^(-i))
    ri = r * eta^i
    result = rbind(result, c(smax - s + 1, i + 1, ri, ni))
  }
}

names(result) = c("bracket", "stage", "budget", "n")

knitr::kable(
  list(
	result[5:7, 3:4],
	result[1:4, 2:4],
	result[8:9, 3:4],
	result[10, 3:4]
  ),
  booktabs = TRUE,
  caption = "Bracket layout for $\\eta = 2$ and $R = 8$"
)
```

Of course, early termination based on a performance criterion may be disadvantageous if it is done too aggressively in certain scenarios.
A learner with hyperparameters prone to radically jump in their performance estimation during training phase may get configurations canceled too early, simply because they do not improve quick enough compared to others.
In other words, it is often unclear beforehand if having an high amount of configurations n, that gets aggressively discarded early, is better than having a high budget B per configuration.
The arising tradeoff, that has to be made, is called the "$n$ versus $B/n$" problem.
To create a balance between exploitation by selection based on early training performance versus exploration of training performances in later training stages, several brackets are constructed with an associated set of configurations of varying sizes.
Different brackets are initialized with different number of configurations, and different initial budget sizes for the first stage, but each bracket is assigned (approximately) the same budget.
Some brackets contain many configurations, with a small initial budget, so that many are discarded after having been trained for only a short amount of time.
Others are constructed with few configurations, where discarding only takes place after a significant amount of budget was consumed.
The former bracket corresponds to the exploitation (or high $n$), while the latter to the exploration (or high $B/n$).

The configurations at the start of each bracket are initialized by stochastic, often uniform, sampling.
Each bracket is divided into multiple stages, with a stage acting as a termination judge.
The first stage allocates a small budget (the more configurations to start with, the smaller the budget) to each configuration to then discard the worst performing ones, after the budget is consumed.
The fraction $1/\eta$ of discarded configurations in each stage is given by the user in the form of a numeric argument eta bigger than `1`.
The following stage then takes the survivors and allocates additional budget to them (less survivors means more budget for each), and the survival game starts again.
Because the whole budget of a bracket is equally split up between the stages, the budget of a single (surviving) configuration is increased by a factor of $\eta$ between follow-up stages.
This runs until the budget of a bracket is consumed.
To end up with only a small amount of survivors after the last stage terminates, the amount of configurations to start with is calculated at the start of each bracket.
Note that currently all configurations are trained completely from the beginning, so no online updates of models from stage to stage is happening.

To identify the budget for evaluating hyperband, the user has to specify explicitly which hyperparameter of the learner influences the budget by extending a single hyperparameter in the `r ref("ParamSet")` with an argument (`tags = "budget"`), like in the following snippet:

```{r 03-optimization-012}
library(paradox)

# Hyperparameter subset of XGBoost
params = list(
  ParamInt$new("nrounds", lower = 1, upper = 16, tags = "budget"),
  ParamFct$new("booster", levels = c("gbtree", "gblinear", "dart"))
)
```

Thanks to the broad ecosystem of the `r mlr_pkg("mlr3verse")` a learner does not require a natural budget parameter.
An alternative approach using subsampling as preprocessing with `r mlr_pkg("mlr3pipelines")` is described below.
This makes up a lacking budget parameter in decision trees.

```{r 03-optimization-013}
library(mlr3hyperband)
library(mlr3pipelines)
set.seed(123)

# extend "classif.rpart" with "subsampling" as preprocessing step
ll = po("subsample") %>>% lrn("classif.rpart")

# extend hyperparameters of "classif.rpart" with subsampling fraction as budget
params = list(
  ParamDbl$new("classif.rpart.cp", lower = 0.001, upper = 0.1),
  ParamInt$new("classif.rpart.minsplit", lower = 1, upper = 10),
  ParamDbl$new("subsample.frac", lower = 0.1, upper = 1, tags = "budget")
)
```

We can now plug the new learner with the extended hyperparameter set into a `r ref("TuningInstance")` the same way as usual.
Naturally, Hyperband terminates once all of its brackets are evaluated, so a `r ref("Terminator")` in the tuning instance acts as an upper bound and should be only set to a low value if one is unsure of how long hyperband will take to finish under the given settings.

```{r 03-optimization-014}
inst = TuningInstance$new(
  tsk("iris"),
  ll,
  rsmp("holdout"),
  msr("classif.ce"),
  ParamSet$new(params),
  term("evals", n_evals = 100000L)
)
```

Now, we initialize a new instance of the `r ref("mlr3hyperband::TunerHyperband")` class and start tuning with it.

```{r 03-optimization-015}
tuner = TunerHyperband$new(eta = 3)
tuner$tune(inst)
```

To receive the results of each sampled configuration, we simply run the following snippet.

```{r 03-optimization-016}
inst$archive(unnest = "params")[, c(
  "subsample.frac",
  "classif.rpart.cp",
  "classif.rpart.minsplit",
  "classif.ce"
)]
```

Additionally, we can also only extract the best performing configuration.
Note, that we receive a `r ref("data.table")` object above, but a `r ref("ResampleResult")` object below.

```{r 03-optimization-017}
inst$best()
```

If you are familiar with the original paper, you may have wondered how we just used Hyperband with a parameter ranging from `0.1` to `1.0`.
The answer is, with the help the internal rescaling of the budget parameter.
`r gh_pkg("mlr-org/mlr3hyperband")` automatically divides the budget parameters boundaries with its lower bound, ending up with a budget range starting again at `1`, like it is the case originally.
If we want an overview of what bracket layout hyperband created to perform its tuning and how the rescaling in each bracket worked, we can print a compact table to see this.

```{r 03-optimization-018}
tuner$info
```

In the traditional way, hyperband uses uniform sampling to receive a configuration sample at the start of each bracket.
But it is also possible to define a custom `r ref("Sampler")` for each hyperparameter.

```{r 03-optimization-019}
library(mlr3learners)
set.seed(123)

params = list(
  ParamInt$new("nrounds", lower = 1, upper = 16, tag = "budget"),
  ParamDbl$new("eta",     lower = 0, upper = 1),
  ParamFct$new("booster", levels = c("gbtree", "gblinear", "dart"))
)

inst = TuningInstance$new(
  tsk("iris"),
  lrn("classif.xgboost"),
  rsmp("holdout"),
  msr("classif.ce"),
  ParamSet$new(params),
  term("evals", n_evals = 100000L)
)

# beta distribution with alpha = 2 and beta = 5
# categorical distribution with custom probabilities
sampler = SamplerJointIndep$new(list(
  Sampler1DRfun$new(params[[2]], function(n) rbeta(n, 2, 5)),
  Sampler1DCateg$new(params[[3]], prob = c(0.2, 0.3, 0.5))
))
```

Then, the defined sampler has to be given as an argument during instance creation.
Afterwards, the usual tuning can proceed.

```{r 03-optimization-020}
tuner = TunerHyperband$new(eta = 2, sampler = sampler)
tuner$tune(inst)

inst$best()
```

Furthermore, we extended the original alogrithm, to make it also possible to use `r gh_pkg("mlr-org/mlr3hyperband")` for multi-objective optimization.
To do this, simply specify more measures in the tuninginstance and run the rest as usual.

```{r 03-optimization-021}
inst = TuningInstance$new(
  tsk("pima"),
  lrn("classif.xgboost"),
  rsmp("holdout"),
  list(msr("classif.tpr"), msr("classif.fpr")),
  ParamSet$new(params),
  term("evals", n_evals = 100000L)
)

tuner = TunerHyperband$new(eta = 4)
tuner$tune(inst)
```

Keep in mind that `$best()` may not work as desired in the multi-objective case as it always returns only one result w.r.t. a single measure.


## Feature Selection / Filtering {#fs}

Often, data sets include a large number of features.
The technique of extracting a subset of relevant features is called "feature selection".
The objective of feature selection is to fit the sparse dependent of a model on a subset of available data features in the most suitable manner.
Feature selection can enhance the interpretability of the model, speed up the learning process and improve the learner performance.
Different approaches exist to identify the relevant features.
In the literature two different approaches are emphasized:
One is called [Filtering](#fs-filtering) and the other approach is often referred to as feature subset selection or [wrapper methods](#fs-wrapper).

What are the differences [@chandrashekar2014]?

* **Filter**: An external algorithm computes a rank of the variables (e.g. based on the correlation to the response).
  Then, features are subsetted by a certain criteria, e.g. an absolute number or a percentage of the number of variables.
  The selected features will then be used to fit a model (with optional hyperparameters selected by tuning).
  This calculation is usually cheaper than “feature subset selection” in terms of computation time.
* **Feature subset selection**: Here, no ranking of features is done.
  Features are selected by a (random) subset of the data.
  Then, a model is fitted and the performance is checked.
  This is done for a lot of feature combinations in a cross-validation (CV) setting and the best combination is reported.
  This method is very computational intense as a lot of models are fitted.
  Also, strictly all these models would need to be tuned before the performance is estimated which would require an additional nested level in a CV setting.
  After all this, the selected subset of features is again fitted (with optional hyperparameters selected by tuning).

There is also a third approach which can be attributed to the "filter" family:
The embedded feature-selection methods of some `r ref("Learner")`.
Read more about how to use these in section [embedded feature-selection methods](#fs-embedded).

[Ensemble filters]({#fs-ensemble}) built upon the idea of stacking single filter methods.
These are not yet implemented.

All feature selection related functionality is implemented via the extension package `r gh_pkg("mlr-org/mlr3filters")`.

### Filters {#fs-filter}

Filter methods assign an importance value to each feature.
Based on these values the features can be ranked and a feature subset can be selected.
There is a list of all implemented filter methods in the [Appendix](#list-filters).

### Calculating filter values {#fs-calc}

Currently, only classification and regression tasks are supported.

The first step it to create a new R object using the class of the desired filter method.
Each object of class `Filter` has a `.$calculate()` method which calculates the filter values and ranks them in a descending order.

```{r 03-optimization-022}
library(mlr3filters)
filter = FilterJMIM$new()

task = tsk("iris")
filter$calculate(task)

as.data.table(filter)
```

Some filters support changing specific hyperparameters.
This is done similar to setting hyperparameters of a `r ref("Learner")` using `.$param_set$values`:

```{r 03-optimization-023}
filter_cor = FilterCorrelation$new()
filter_cor$param_set

# change parameter 'method'
filter_cor$param_set$values = list(method = "spearman")
filter_cor$param_set
```

Rather than taking the "long" R6 way to create a filter, there is also a built-in shorthand notation for filter creation

```{r 03-optimization-024}
filter = flt("cmim")
filter
```

### Variable Importance Filters {#fs-var-imp-filters}

All `r ref("Learner")` with the property "importance" come with integrated feature selection methods.

You can find a list of all learners with this property in the [Appendix](#fs-filter-embedded-list).

For some learners the desired filter method needs to be set during learner creation.
For example, learner `classif.ranger` (in `r mlr_pkg("mlr3learners")` comes with multiple integrated methods.
See the help page of `r ref("ranger::ranger")`.
To use method "impurity", you need to set the filter method during construction.

```{r 03-optimization-025}
library(mlr3learners)
lrn = lrn("classif.ranger", importance = "impurity")
```

Now you can use the `r ref("mlr3filters::FilterImportance")` class for algorithm-embedded methods to filter a `r ref("Task")`.

```{r 03-optimization-026}
library(mlr3learners)

task = tsk("iris")
filter = flt("importance", learner = lrn)
filter$calculate(task)
head(as.data.table(filter), 3)
```

### Ensemble Methods {#fs-ensemble}

```{block, type='warning'}
Work in progress :)
```

### Wrapper Methods {#fs-wrapper}

```{block, type='warning'}
Work in progress :) - via package _mlr3fswrap_
```

## Nested Resampling {#nested-resampling}

In order to obtain unbiased performance estimates for learners, all parts of the model building (preprocessing and model selection steps) should be included in the resampling, i.e., repeated for every pair of training/test data.
For steps that themselves require resampling like hyperparameter tuning or feature-selection (via the wrapper approach) this results in two nested resampling loops.

```{r 03-optimization-027, echo = FALSE}
knitr::include_graphics("images/nested_resampling.png")
```

The graphic above illustrates nested resampling for parameter tuning with 3-fold cross-validation in the outer and 4-fold cross-validation in the inner loop.

In the outer resampling loop, we have three pairs of training/test sets.
On each of these outer training sets parameter tuning is done, thereby executing the inner resampling loop.
This way, we get one set of selected hyperparameters for each outer training set. Then the learner is fitted on each outer training set using the corresponding selected hyperparameters and its performance is evaluated on the outer test sets.

In `r gh_pkg("mlr-org/mlr3")`, you can get nested resampling for free without programming any looping by using the `r ref("mlr3tuning::AutoTuner")` class.
This works as follows:

1. Generate a wrapped Learner via class `r ref("mlr3tuning::AutoTuner")` or `mlr3filters::AutoSelect` (not yet implemented).
2. Specify all required settings - see section ["Automating the Tuning"](#autotuner) for help.
3. Call function `r ref("resample()")` or `r ref("benchmark()")` with the created `r ref("Learner")`.

You can freely combine different inner and outer resampling strategies.

A common setup is prediction and performance evaluation on a fixed outer test set. This can be achieved by passing the `r ref("Resampling")` strategy (`rsmp("holdout")`) as the outer resampling instance to either `r ref("resample()")` or `r ref("benchmark()")`.

The inner resampling strategy could be a cross-validation one (`rsmp("cv")`) as the sizes of the outer training sets might differ.
Per default, the inner resample description is instantiated once for every outer training set.

Nested resampling is computationally expensive.
For this reason in the examples shown below, we use relatively small search spaces and a low number of resampling iterations.
In practice, you normally have to increase both.
As this is computationally intensive you might want to have a look at the section [Parallelization](#parallelization).

### Execution

To optimize hyperparameters or conduct features-selection in a nested resampling you need to create learners using either:

* the `r ref("AutoTuner")` class, or
* the `mlr3filters::AutoSelect` class (not yet implemented)

We use the example from section ["Automating the Tuning"](#autotuner) and pipe the resulting learner into a `r ref("resample()")` call.

```{r 03-optimization-028}
library(mlr3tuning)
task = tsk("iris")
learner = lrn("classif.rpart")
resampling = rsmp("holdout")
measures = msr("classif.ce")
param_set = paradox::ParamSet$new(
  params = list(paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1)))
terminator = term("evals", n_evals = 5)
tuner = tnr("grid_search", resolution = 10)

at = AutoTuner$new(learner, resampling, measures = measures,
  param_set, terminator, tuner = tuner)
```

Now construct the `r ref("resample()")` call:

```{r 03-optimization-029}
resampling_outer = rsmp("cv", folds = 3)
rr = resample(task = task, learner = at, resampling = resampling_outer)
```

### Evaluation {#rr-eval}

With the created `r ref("ResampleResult")` we can now inspect the executed resampling iterations more closely.
See also section [Resampling](#resampling) for more detailed information about `r ref("ResampleResult")` objects.

For example, we can query the aggregated performance result:

```{r 03-optimization-030}
rr$aggregate()
```

<!-- We can also query the tuning result of any learner using the `$tune_path` field of the `r ref("AutoTuner")` class stored in the `r ref("ResampleResult")` container `rr`. -->

<!-- ```{block, type="caution"} -->
<!-- Note: This only works if `store_bmr` was set to `TRUE` in the `AutoTuner` object. -->
<!-- ``` -->

<!-- ```{r 02-nested-resamp-005, eval = FALSE} -->
<!-- # FIXME: not yet done -->
<!-- rr$learners[[1]]$tune_path -->
<!-- ``` -->

Check for any errors in the folds during execution (if there is not output, warnings or errors recorded, this is an empty `data.table()`:

```{r 03-optimization-031}
rr$errors
```

Or take a look at the confusion matrix of the joined predictions:

```{r 03-optimization-032}
rr$prediction()$confusion
```

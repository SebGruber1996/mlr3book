## Hyperparameter Tuning {#tuning}

Hyperparameter tuning is supported via the extension package `r mlr_pkg("mlr3tuning")`.
The heart of `r mlr_pkg("mlr3tuning")` are the R6 classes

* `r ref("TuningInstance")`: Describes the tuning problem and stores results.
* `r ref("Tuner")`: Base class for implementations of tuning algorithms.

### The `TuningInstance` Class

The following sub-section examines the optimization of a simple classification tree on the `r ref("mlr_tasks_pima", text = "Pima Indian Diabetes")` data set.

```{r 01-tuning-001}
task = tsk("pima")
print(task)
```

We use the classification tree from `r cran_pkg("rpart")` and choose a subset of the hyperparameters we want to tune.
This is often referred to as the "tuning space".

```{r 01-tuning-002}
learner = lrn("classif.rpart")
learner$param_set
```

Here, we opt to tune two parameters: the complexity `cp` and the termination criterion `minsplit`.
As the tuning space has to be bound, one has to set lower and upper bounds:

```{r 01-tuning-003}
library(paradox)
tune_ps = ParamSet$new(list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)
))
tune_ps
```

Next, we need to define how to evaluate the performance.
For this, we need to choose a `r ref("Resampling", text = "resampling strategy")` and a `r ref("Measure", text = "performance measure")`.

```{r 01-tuning-004}
hout = rsmp("holdout")
measure = msr("classif.ce")
```

Finally, one has to determine the budget available to solve this tuning instance.
This is done by selecting one of the available `r ref("Terminator", text = "Terminators")`:

- Terminate after a given time (`r ref("TerminatorClockTime")`)
- Terminate after a given amount of iterations (`r ref("TerminatorEvals")`)
- Terminate after a specific performance is reached (`r ref("TerminatorPerfReached")`)
- A combination of the above in an *ALL* or *ANY* fashion, using `r ref("TerminatorCombo")`

For this short introduction, we grant a budget of 20 evaluations and then put everything together into a `r ref("TuningInstance")`:

```{r 01-tuning-005}
library(mlr3tuning)

evals20 = term("evals", n_evals = 20)

instance = TuningInstance$new(
  task = task,
  learner = learner,
  resampling = hout,
  measures = measure,
  param_set = tune_ps,
  terminator = evals20
)
print(instance)
```

To start the tuning, we still need to select how the optimization should take place - in other words, we need to choose the **optimization algorithm** via the `r ref("Tuner")` class.

### The `Tuner` Class

The following algorithms are currently implemented in `r mlr_pkg("mlr3tuning")`:

- Grid Search (`r ref("TunerGridSearch")`)
- Random Search (`r ref("TunerRandomSearch")`) [@bergstra2012]
- Generalized Simulated Annealing (`r ref("TunerGenSA")`)

In this example we will use a simple grid search with a grid resolution of 10:

```{r 01-tuning-006}
tuner = tnr("grid_search", resolution = 5)
```

Since we have only numeric parameters, `r ref("TunerGridSearch")` will create a grid of equally-sized steps between the respective upper and lower bounds.
As we have two hyperparameters with a resolution of 5, the two-dimensional grid consists of $5^2 = 25$ configurations.
Each configuration serves as hyperparameter setting for the classification tree and triggers a 3-fold cross validation on the task.
All configurations will be examined by the tuner (in a random order), until either all configurations are evaluated or the `r ref("Terminator")` signals that the budget is exhausted.

### Triggering the Tuning

To start the tuning, we simply pass the `r ref("TuningInstance")` to the `$tune()` method of the initialized `r ref("Tuner")`.
The tuner proceeds as follow:

1. The `r ref("Tuner")` proposes at least one hyperparameter configuration (the `r ref("Tuner")` and may propose multiple points to improve parallelization, which can be controlled via the setting `batch_size`).
2. For each configuration, a `r ref("Learner")` is fitted on `r ref("Task")` using the provided `r ref("Resampling")`.
   The results are combined with other results from previous iterations to a single `r ref("BenchmarkResult")`.
3. The `r ref("Terminator")` is queried if the budget is exhausted.
   If the budget is not exhausted, restart with 1) until it is.
4. Determine the configuration with the best observed performance.
5. Return a named list with the hyperparameter settings (`"values"`) and the corresponding measured performance (`"performance"`).

```{r 01-tuning-007}
result = tuner$tune(instance)
print(result)
```

We can investigate all resamplings which where undertaken, using the `$archive()` method of the `r ref("TuningInstance")`.
Here, one just extracts the performance values and the hyperparameters:

```{r 01-tuning-008}
instance$archive()[, .(cp = sapply(params, "[", "cp"), minsplit = sapply(params, "[", "minsplit"), classif.ce)]
```

In total, the grid search evaluated 20/25 different configurations of the grid in a random order before the `r ref("Terminator")` stopped the tuning.

Now the optimized hyperparameters can take the previously created `r ref("Learner")`, set the returned hyperparameters and [train](#train-predict) it on the full dataset.

```{r 01-tuning-009}
learner$param_set$values = instance$result$params
learner$train(task)
```

The trained model could now be used to make a prediction on external data.
Note that predicting on observations present in the `task`, is statistically bias and should be avoided, as the model has seen these observations already during tuning.
Hence, the resulting performance measure would be over-optimistic.
Instead, to get unbiased performance estimates for the current task, [nested resampling](#nested-resamling) is required.

### Automating the Tuning {#autotuner}

The `r ref("AutoTuner")` wraps a learner and augments it with an automatic tuning for a given set of hyperparameters.
Because the `r ref("AutoTuner")` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner.
Analogously to the previous subsection, a new classification tree learner is created.
This classification tree learner automatically tunes the parameters `cp` and `minsplit` using an inner resampling (holdout).
We create a terminator which allows 10 evaluations, and use a simple random search as tuning algorithm:

```{r 01-tuning-010}
library(paradox)
library(mlr3tuning)

learner = lrn("classif.rpart")
resampling = rsmp("holdout")
measures = msr("classif.ce")
tune_ps = ParamSet$new(list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)
))
terminator = term("evals", n_evals = 10)
tuner = tnr("random_search")

at = AutoTuner$new(
  learner = learner,
  resampling = resampling,
  measures = measures,
  tune_ps = tune_ps,
  terminator = terminator,
  tuner = tuner
)
at
```

We can now use the learner like any other learner, calling the `$train()` and `$predict()` method.
This time however, we pass it to `r ref("benchmark()")` to compare the tuner to a classification tree without tuning.
This way, the `r ref("AutoTuner")` will do its resampling for tuning on the training set of the respective split of the outer resampling.
The learner then predicts using the test set of the outer resampling.
This yields unbiased performance measures, as the observations in the test set have not been used during tuning or fitting of the respective learner.
This is called nested resampling.

To compare the tuned learner with the learner using its default, we can use `r ref("benchmark()")`:

```{r 01-tuning-011}
grid = benchmark_grid(
  task = tsk("pima"),
  learner = list(at, lrn("classif.rpart")),
  resampling = rsmp("cv", folds = 3)
)
bmr = benchmark(grid)
bmr$aggregate(measures)
```

Note that we do not expect any differences here compared to the non-tuned approach for multiple reasons:

* the task is too easy
* the task is rather small, and thus prone to overfitting
* the tuning budget (10 evaluations) is small
* `r cran_pkg("rpart")` does not benefit that much from tuning

### Tuning with Hyperband {#hyperband}

Besides the more traditional tuning methods listed above, the mlr3 eco system offers another procedure called hyperband implemented in the mlr3hyperband package.
Hyperband is a budget oriented-procedure, weeding out suboptimally performing configurations early on during their training process, increasing tuning efficiency as a consequence.
For this, several brackets are constructed with an associated set of configurations for each bracket. These configuration are initialized by stochastic, often uniform, sampling.
Each bracket is divided into multiple stages, and configurations are evaluated for a increasing budget in each stage.
Note that currently all configurations are trained completely from the beginning, so no online updates of models.
Once a stage of a bracket is evaluated, the best proportion of \eqn{\frac{1}{\eta}}{1/eta} configurations are kept for the next stage, while the rest is discarded.
The remaining configurations are retrained in the next bracket stage, with budget increased by the factor \eqn{\eta}{eta}.

Different brackets are initialized with different number of configurations, and different initial budget sizes for the first stage, but each bracket is assigned (approximately) the same global summed budget for all of its evaluations.
Some brackets contain many configurations, with a small initial budget, so that many are discarded after having been trained for only a short amount of time.
Others are constructed with few configurations where discarding only takes place after a significant amount of budget.

To identify the budget for evaluating hyperband, the user has to specify explicitly which hyperparameter of the learner influences the budget by tagging a single hyperparameter in the [paradox::ParamSet] with `"budget"`.
An alternative approach using subsampling and pipelines is described below.

Naturally, hyperband terminates once all of its brackets are evaluated, so a [Terminator][mlr3tuning::Terminator] in the tuning instance acts as an upper bound and should be only set to a low value if one is unsure of how long hyperband will take to finish under the given settings.

One is by using the size of the training set as the budget, with the full set as the maximum budget; this is done with the help of [mlr3pipelines]:

```{r  01-tuning-012}
library(mlr3hyperband)
library(mlr3pipelines)
set.seed(123)

# define Graph Learner from rpart with subsampling as preprocessing step
pops = po("subsample")
graph_learner = pops %>>% lrn("classif.rpart")
   
# define with extended hyperparameters with subsampling fraction as budget
# ==> no learner budget is required
params = list(
  ParamDbl$new("classif.rpart.cp", lower = 0.001, upper = 0.1),
  ParamInt$new("classif.rpart.minsplit", lower = 1, upper = 10),
  ParamDbl$new("subsample.frac", lower = 0.1, upper = 1, tags = "budget")
)
   
# define TuningInstance with the Graph Learner and the extended hyperparams
inst = TuningInstance$new(
  tsk("iris"),
  graph_learner,
  rsmp("holdout"),
  msr("classif.ce"),
  ParamSet$new(params),
  term("evals", n_evals = 100000)
)
   
# define and call hyperband as usual
tuner = TunerHyperband$new(eta = 3.5)
expect_tuner(tuner)
tuner$tune(inst)
expect_resample_result(inst$best())

results = inst$archive()[, .(
  frac = sapply(params, "[", "subsample.frac"),
  cp = sapply(params, "[", "classif.rpart.cp"), 
  minsplit = sapply(params, "[", "classif.rpart.minsplit"),
  classif.ce
)]

```

While the other way is by explicitly specifying which learner's hyperparameter is the budget (here, we use XGBoost with `nrounds` as budget parameter):

```{r  01-tuning-013}
library(mlr3learners)
set.seed(123)

# define hyperparameter and budget parameter for tuning with hyperband
params = list(
  ParamInt$new("nrounds", lower = 1, upper = 16, tags = "budget"),
  ParamDbl$new("eta",     lower = 0, upper = 1),
  ParamFct$new("booster", levels = c("gbtree", "gblinear", "dart"))
)

# initialize TuningInstance as usual
inst = TuningInstance$new(
  task = tsk("iris"),
  learner = lrn("classif.xgboost"),
  resampling = rsmp("holdout"),
  measures = msr("classif.ce"),
  ParamSet$new(params),
  term("evals", n_evals = 100000L)
)

# initialize Hyperband Tuner and tune
tuner = TunerHyperband$new(eta = 2L)
result = tuner$tune(inst)

# view results
instance$archive()[, .(nrounds = sapply(params, "[", "nrounds"), cp = sapply(params, "[", "cp"), minsplit = sapply(params, "[", "minsplit"), classif.ce)]
```
